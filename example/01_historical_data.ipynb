{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical Data Aggregation\n",
    "\n",
    "In order to fine-tune the model specifically on shorter time frame data, we want to store historical data for the past year for the set of tickers that we have provided to the client side. The two most important values to define are the sets of tickers as well as the shape of the data for fine-tuning. For convience, we have defined a `_reshape` method on the MoiRai model class, which receives a list of aggregate bars from Polygon, and creates a dataframe of the shape required for fine-tuning. We can use this method in the data aggregation pipeline, in order to create dataframes of the shape we require for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.model import MoiRai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "from polygon import RESTClient\n",
    "from polygon.rest.models import Agg\n",
    "\n",
    "from src._data import AggregateInterval\n",
    "from src.config import SERVER_CONFIG\n",
    "\n",
    "model_class = MoiRai(interval=AggregateInterval.ONE_MINUTE)\n",
    "rest_client = RESTClient(api_key=SERVER_CONFIG.POLYGON_API_KEY)\n",
    "\n",
    "def _fetch_data(ticker: str, date: datetime) -> List[Agg]:\n",
    "    try:\n",
    "        end_date = date.replace(day=date.day + 1)\n",
    "    \n",
    "    # If the date is out of range, return empty list\n",
    "    except ValueError:\n",
    "        return []\n",
    "\n",
    "    start_ts = int(date.timestamp() * 1000)\n",
    "    end_ts = int(end_date.timestamp() * 1000)\n",
    "\n",
    "    result = rest_client.get_aggs(\n",
    "        ticker=ticker,\n",
    "        multiplier=10, # Training on 10 second interval data\n",
    "        timespan=\"second\",\n",
    "        from_=start_ts,\n",
    "        to=end_ts,\n",
    "        sort=\"asc\",\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_ticker_data(ticker: str, month: int, year: int) -> List[Agg]:\n",
    "\n",
    "    aggregates = []\n",
    "    for i in range(1, 32):\n",
    "        try: \n",
    "            date = datetime(year, month, i)\n",
    "\n",
    "        # Catch an exception if the day is out of range\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        data = _fetch_data(ticker, date)\n",
    "        aggregates.extend(data)\n",
    "    return aggregates\n",
    "\n",
    "def process_result(ticker: str, aggregates: List[Agg]):\n",
    "    df = model_class._reshape(aggregates)\n",
    "    df['ticker'] = ticker\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline for processing a single month\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "\n",
    "def aggregate_ticker_historical(ticker: str, year: int):\n",
    "    aggregates = []\n",
    "    for i in range(1, 13):\n",
    "        data = get_ticker_data(ticker, i, year)\n",
    "        aggregates.extend(data)\n",
    "\n",
    "    df = process_result(ticker, aggregates)\n",
    "    return df\n",
    "\n",
    "## Pipeline for processing a single year for all 10 tickers\n",
    "def aggregate_dataset(year: int):\n",
    "    tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", 'AMZN', \"TSLA\", \"NVDA\", \"META\", \"BAC\", \"V\", \"NFLX\"]\n",
    "    dfs = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(aggregate_ticker_historical, tickers, [year] * len(tickers))\n",
    "        for result in results:\n",
    "            dfs.append(result)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = aggregate_dataset(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator, Any\n",
    "from datasets import Features, Sequence, Value\n",
    "\n",
    "def multivar_example_gen_func() -> Generator[dict[str, Any], None, None]:\n",
    "    # Group the dataframe by the different ticker symbols\n",
    "    for item_id, ticker_df in df.groupby(\"ticker\"):\n",
    "        ticker_df.drop(columns=[\"ticker\"], inplace=True)\n",
    "        \n",
    "\n",
    "        # Group the ticker_df dataframe by day\n",
    "        for date, daily_df in ticker_df.groupby(ticker_df.index.date):\n",
    "\n",
    "            daily_df = daily_df.resample(\"10s\").last()\n",
    "            # Make sure the daily_df has at least 3 data points\n",
    "            try:\n",
    "                freq = pd.infer_freq(daily_df.index)\n",
    "            except ValueError:\n",
    "                freq = None\n",
    "\n",
    "            yield {\n",
    "                \"target\": daily_df.to_numpy().T,  # array of shape (var, time)\n",
    "                \"start\": daily_df.index[0],\n",
    "                \"freq\": \"10s\",\n",
    "                \"item_id\": f\"{item_id}_{date}\",\n",
    "            }\n",
    "\n",
    "def aapl_example_gen_func() -> Generator[dict[str, Any], None, None]:\n",
    "    # Filter the dataset down to just the AAPL ticker\n",
    "    aapl_df = df[df[\"ticker\"] == \"AAPL\"]\n",
    "    aapl_df.drop(columns=[\"ticker\"], inplace=True)\n",
    "\n",
    "    aapl_df = aapl_df.resample(\"10s\").last()\n",
    "\n",
    "    freq = pd.infer_freq(aapl_df.index)\n",
    "\n",
    "    print(freq)\n",
    "    yield {\n",
    "        \"target\": aapl_df.to_numpy().T,  # array of shape (var, time)\n",
    "        \"start\": aapl_df.index[0],\n",
    "        \"freq\": freq,\n",
    "        \"item_id\": \"AAPL\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features(\n",
    "        dict(\n",
    "            item_id=Value(\"string\"),\n",
    "            start=Value(\"timestamp[s]\"),\n",
    "            freq=Value(\"string\"),\n",
    "            target=Sequence(Sequence(Value(\"float32\")), length=len(df.columns) - 1),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2371 examples [00:27, 87.63 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1896/1896 [00:00<00:00, 21149.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 237/237 [00:00<00:00, 27459.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 238/238 [00:00<00:00, 28522.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "hf_dataset = datasets.Dataset.from_generator(\n",
    "    multivar_example_gen_func, features=features\n",
    ")\n",
    "\n",
    "# aapl_dataset = datasets.Dataset.from_generator(\n",
    "#     aapl_example_gen_func, features=features\n",
    "# )\n",
    "\n",
    "# aapl_dataset.save_to_disk(\"datasets/candles_10sec_aapl\")\n",
    "# Keep 80% of the data for training, 10% for validation, and 10% for testing\n",
    "train_testvalid = hf_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Keep the 80% split for training\n",
    "train_set = train_testvalid['train']\n",
    "\n",
    "# Split the remaining 20% in half for validation and testing\n",
    "val_test = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# Assign the validation and testing sets\n",
    "val_set = val_test['train']\n",
    "test_set = val_test['test']\n",
    "\n",
    "train_set.save_to_disk(\"datasets/candles_10sec_04222024_train\")\n",
    "val_set.save_to_disk(\"datasets/candles_10sec_04222024_val\")\n",
    "test_set.save_to_disk(\"datasets/candles_10sec_04222024_test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Approach For Dataset\n",
    "\n",
    "In order to reproduce the dataset used for finetuning, we will attempt to reproduce the steps used in the finetuning example provided in the `README.md` of the `uni2ts` repository. This will include:\n",
    "- Fetching 1 month of intraday data from AAPL\n",
    "- Resampling the dataframe, to adhere to a 10s frequency\n",
    "- Cleaning the data, to fill in the NaN values, with logical fillers\n",
    "    - Should have some way of logically filling in NaN values, likely just by taking the average between the two candles, and having no change in the candle (same next candle), making sure the close value corresponds to the next open value\n",
    "- Store data in CSV file\n",
    "- Mimic steps used in the "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monger-inference-qa09yyar-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
